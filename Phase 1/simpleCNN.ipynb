{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m LeNet()\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1161\u001b[0m         device,\n\u001b[1;32m   1162\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1163\u001b[0m         non_blocking,\n\u001b[1;32m   1164\u001b[0m     )\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transforms (CIFAR-100 images are 3-channel RGB, so we'll convert to grayscale for the LeNet input)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert RGB to grayscale\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32 if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize grayscale images (mean, std)\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Now your model can be trained with train_loader and evaluated with test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/1563], Loss: 2.2294\n",
      "Epoch [1/20], Step [200/1563], Loss: 2.0515\n",
      "Epoch [1/20], Step [300/1563], Loss: 1.8967\n",
      "Epoch [1/20], Step [400/1563], Loss: 1.7837\n",
      "Epoch [1/20], Step [500/1563], Loss: 1.7239\n",
      "Epoch [1/20], Step [600/1563], Loss: 1.7259\n",
      "Epoch [1/20], Step [700/1563], Loss: 1.6775\n",
      "Epoch [1/20], Step [800/1563], Loss: 1.6669\n",
      "Epoch [1/20], Step [900/1563], Loss: 1.6354\n",
      "Epoch [1/20], Step [1000/1563], Loss: 1.6249\n",
      "Epoch [1/20], Step [1100/1563], Loss: 1.6530\n",
      "Epoch [1/20], Step [1200/1563], Loss: 1.6037\n",
      "Epoch [1/20], Step [1300/1563], Loss: 1.6188\n",
      "Epoch [1/20], Step [1400/1563], Loss: 1.5667\n",
      "Epoch [1/20], Step [1500/1563], Loss: 1.5415\n",
      "Epoch [2/20], Step [100/1563], Loss: 1.5285\n",
      "Epoch [2/20], Step [200/1563], Loss: 1.5206\n",
      "Epoch [2/20], Step [300/1563], Loss: 1.4896\n",
      "Epoch [2/20], Step [400/1563], Loss: 1.4862\n",
      "Epoch [2/20], Step [500/1563], Loss: 1.4922\n",
      "Epoch [2/20], Step [600/1563], Loss: 1.4875\n",
      "Epoch [2/20], Step [700/1563], Loss: 1.4548\n",
      "Epoch [2/20], Step [800/1563], Loss: 1.4157\n",
      "Epoch [2/20], Step [900/1563], Loss: 1.4536\n",
      "Epoch [2/20], Step [1000/1563], Loss: 1.4591\n",
      "Epoch [2/20], Step [1100/1563], Loss: 1.4659\n",
      "Epoch [2/20], Step [1200/1563], Loss: 1.3886\n",
      "Epoch [2/20], Step [1300/1563], Loss: 1.3681\n",
      "Epoch [2/20], Step [1400/1563], Loss: 1.4091\n",
      "Epoch [2/20], Step [1500/1563], Loss: 1.4216\n",
      "Epoch [3/20], Step [100/1563], Loss: 1.3097\n",
      "Epoch [3/20], Step [200/1563], Loss: 1.3678\n",
      "Epoch [3/20], Step [300/1563], Loss: 1.3645\n",
      "Epoch [3/20], Step [400/1563], Loss: 1.3297\n",
      "Epoch [3/20], Step [500/1563], Loss: 1.3768\n",
      "Epoch [3/20], Step [600/1563], Loss: 1.3127\n",
      "Epoch [3/20], Step [700/1563], Loss: 1.3282\n",
      "Epoch [3/20], Step [800/1563], Loss: 1.3178\n",
      "Epoch [3/20], Step [900/1563], Loss: 1.3217\n",
      "Epoch [3/20], Step [1000/1563], Loss: 1.3152\n",
      "Epoch [3/20], Step [1100/1563], Loss: 1.3154\n",
      "Epoch [3/20], Step [1200/1563], Loss: 1.3027\n",
      "Epoch [3/20], Step [1300/1563], Loss: 1.2850\n",
      "Epoch [3/20], Step [1400/1563], Loss: 1.3465\n",
      "Epoch [3/20], Step [1500/1563], Loss: 1.2550\n",
      "Epoch [4/20], Step [100/1563], Loss: 1.2589\n",
      "Epoch [4/20], Step [200/1563], Loss: 1.2678\n",
      "Epoch [4/20], Step [300/1563], Loss: 1.1977\n",
      "Epoch [4/20], Step [400/1563], Loss: 1.2302\n",
      "Epoch [4/20], Step [500/1563], Loss: 1.2582\n",
      "Epoch [4/20], Step [600/1563], Loss: 1.1945\n",
      "Epoch [4/20], Step [700/1563], Loss: 1.2354\n",
      "Epoch [4/20], Step [800/1563], Loss: 1.2356\n",
      "Epoch [4/20], Step [900/1563], Loss: 1.2509\n",
      "Epoch [4/20], Step [1000/1563], Loss: 1.2080\n",
      "Epoch [4/20], Step [1100/1563], Loss: 1.1867\n",
      "Epoch [4/20], Step [1200/1563], Loss: 1.1819\n",
      "Epoch [4/20], Step [1300/1563], Loss: 1.2133\n",
      "Epoch [4/20], Step [1400/1563], Loss: 1.2182\n",
      "Epoch [4/20], Step [1500/1563], Loss: 1.2393\n",
      "Epoch [5/20], Step [100/1563], Loss: 1.1640\n",
      "Epoch [5/20], Step [200/1563], Loss: 1.1446\n",
      "Epoch [5/20], Step [300/1563], Loss: 1.1481\n",
      "Epoch [5/20], Step [400/1563], Loss: 1.1739\n",
      "Epoch [5/20], Step [500/1563], Loss: 1.1352\n",
      "Epoch [5/20], Step [600/1563], Loss: 1.1350\n",
      "Epoch [5/20], Step [700/1563], Loss: 1.1617\n",
      "Epoch [5/20], Step [800/1563], Loss: 1.1619\n",
      "Epoch [5/20], Step [900/1563], Loss: 1.1564\n",
      "Epoch [5/20], Step [1000/1563], Loss: 1.1688\n",
      "Epoch [5/20], Step [1100/1563], Loss: 1.1424\n",
      "Epoch [5/20], Step [1200/1563], Loss: 1.1497\n",
      "Epoch [5/20], Step [1300/1563], Loss: 1.1899\n",
      "Epoch [5/20], Step [1400/1563], Loss: 1.1484\n",
      "Epoch [5/20], Step [1500/1563], Loss: 1.1016\n",
      "Epoch [6/20], Step [100/1563], Loss: 1.1113\n",
      "Epoch [6/20], Step [200/1563], Loss: 1.0903\n",
      "Epoch [6/20], Step [300/1563], Loss: 1.1021\n",
      "Epoch [6/20], Step [400/1563], Loss: 1.0861\n",
      "Epoch [6/20], Step [500/1563], Loss: 1.0744\n",
      "Epoch [6/20], Step [600/1563], Loss: 1.0467\n",
      "Epoch [6/20], Step [700/1563], Loss: 1.1130\n",
      "Epoch [6/20], Step [800/1563], Loss: 1.1055\n",
      "Epoch [6/20], Step [900/1563], Loss: 1.0557\n",
      "Epoch [6/20], Step [1000/1563], Loss: 1.0612\n",
      "Epoch [6/20], Step [1100/1563], Loss: 1.1343\n",
      "Epoch [6/20], Step [1200/1563], Loss: 1.0874\n",
      "Epoch [6/20], Step [1300/1563], Loss: 1.1139\n",
      "Epoch [6/20], Step [1400/1563], Loss: 1.0806\n",
      "Epoch [6/20], Step [1500/1563], Loss: 1.1149\n",
      "Epoch [7/20], Step [100/1563], Loss: 1.0060\n",
      "Epoch [7/20], Step [200/1563], Loss: 1.0188\n",
      "Epoch [7/20], Step [300/1563], Loss: 1.0582\n",
      "Epoch [7/20], Step [400/1563], Loss: 1.0432\n",
      "Epoch [7/20], Step [500/1563], Loss: 1.0346\n",
      "Epoch [7/20], Step [600/1563], Loss: 1.0190\n",
      "Epoch [7/20], Step [700/1563], Loss: 1.0314\n",
      "Epoch [7/20], Step [800/1563], Loss: 1.0729\n",
      "Epoch [7/20], Step [900/1563], Loss: 1.0109\n",
      "Epoch [7/20], Step [1000/1563], Loss: 1.0521\n",
      "Epoch [7/20], Step [1100/1563], Loss: 1.0348\n",
      "Epoch [7/20], Step [1200/1563], Loss: 1.0515\n",
      "Epoch [7/20], Step [1300/1563], Loss: 1.0617\n",
      "Epoch [7/20], Step [1400/1563], Loss: 1.0658\n",
      "Epoch [7/20], Step [1500/1563], Loss: 1.0406\n",
      "Epoch [8/20], Step [100/1563], Loss: 0.9978\n",
      "Epoch [8/20], Step [200/1563], Loss: 0.9594\n",
      "Epoch [8/20], Step [300/1563], Loss: 0.9751\n",
      "Epoch [8/20], Step [400/1563], Loss: 0.9843\n",
      "Epoch [8/20], Step [500/1563], Loss: 1.0230\n",
      "Epoch [8/20], Step [600/1563], Loss: 0.9968\n",
      "Epoch [8/20], Step [700/1563], Loss: 0.9812\n",
      "Epoch [8/20], Step [800/1563], Loss: 1.0012\n",
      "Epoch [8/20], Step [900/1563], Loss: 0.9734\n",
      "Epoch [8/20], Step [1000/1563], Loss: 1.0140\n",
      "Epoch [8/20], Step [1100/1563], Loss: 0.9889\n",
      "Epoch [8/20], Step [1200/1563], Loss: 0.9964\n",
      "Epoch [8/20], Step [1300/1563], Loss: 1.0130\n",
      "Epoch [8/20], Step [1400/1563], Loss: 0.9762\n",
      "Epoch [8/20], Step [1500/1563], Loss: 1.0250\n",
      "Epoch [9/20], Step [100/1563], Loss: 0.9468\n",
      "Epoch [9/20], Step [200/1563], Loss: 0.9163\n",
      "Epoch [9/20], Step [300/1563], Loss: 0.9164\n",
      "Epoch [9/20], Step [400/1563], Loss: 0.9616\n",
      "Epoch [9/20], Step [500/1563], Loss: 0.9356\n",
      "Epoch [9/20], Step [600/1563], Loss: 0.9502\n",
      "Epoch [9/20], Step [700/1563], Loss: 0.9557\n",
      "Epoch [9/20], Step [800/1563], Loss: 0.9673\n",
      "Epoch [9/20], Step [900/1563], Loss: 1.0057\n",
      "Epoch [9/20], Step [1000/1563], Loss: 0.9510\n",
      "Epoch [9/20], Step [1100/1563], Loss: 0.9404\n",
      "Epoch [9/20], Step [1200/1563], Loss: 0.9604\n",
      "Epoch [9/20], Step [1300/1563], Loss: 0.9898\n",
      "Epoch [9/20], Step [1400/1563], Loss: 0.9906\n",
      "Epoch [9/20], Step [1500/1563], Loss: 0.9680\n",
      "Epoch [10/20], Step [100/1563], Loss: 0.9168\n",
      "Epoch [10/20], Step [200/1563], Loss: 0.8864\n",
      "Epoch [10/20], Step [300/1563], Loss: 0.9008\n",
      "Epoch [10/20], Step [400/1563], Loss: 0.9196\n",
      "Epoch [10/20], Step [500/1563], Loss: 0.9097\n",
      "Epoch [10/20], Step [600/1563], Loss: 0.9314\n",
      "Epoch [10/20], Step [700/1563], Loss: 0.9150\n",
      "Epoch [10/20], Step [800/1563], Loss: 0.9232\n",
      "Epoch [10/20], Step [900/1563], Loss: 0.9305\n",
      "Epoch [10/20], Step [1000/1563], Loss: 0.9296\n",
      "Epoch [10/20], Step [1100/1563], Loss: 0.9201\n",
      "Epoch [10/20], Step [1200/1563], Loss: 0.9235\n",
      "Epoch [10/20], Step [1300/1563], Loss: 0.9317\n",
      "Epoch [10/20], Step [1400/1563], Loss: 0.9275\n",
      "Epoch [10/20], Step [1500/1563], Loss: 0.9321\n",
      "Epoch [11/20], Step [100/1563], Loss: 0.8552\n",
      "Epoch [11/20], Step [200/1563], Loss: 0.8434\n",
      "Epoch [11/20], Step [300/1563], Loss: 0.8883\n",
      "Epoch [11/20], Step [400/1563], Loss: 0.9111\n",
      "Epoch [11/20], Step [500/1563], Loss: 0.8698\n",
      "Epoch [11/20], Step [600/1563], Loss: 0.8848\n",
      "Epoch [11/20], Step [700/1563], Loss: 0.9325\n",
      "Epoch [11/20], Step [800/1563], Loss: 0.8967\n",
      "Epoch [11/20], Step [900/1563], Loss: 0.9056\n",
      "Epoch [11/20], Step [1000/1563], Loss: 0.9174\n",
      "Epoch [11/20], Step [1100/1563], Loss: 0.8527\n",
      "Epoch [11/20], Step [1200/1563], Loss: 0.8892\n",
      "Epoch [11/20], Step [1300/1563], Loss: 0.9166\n",
      "Epoch [11/20], Step [1400/1563], Loss: 0.8988\n",
      "Epoch [11/20], Step [1500/1563], Loss: 0.9077\n",
      "Epoch [12/20], Step [100/1563], Loss: 0.8042\n",
      "Epoch [12/20], Step [200/1563], Loss: 0.8395\n",
      "Epoch [12/20], Step [300/1563], Loss: 0.8428\n",
      "Epoch [12/20], Step [400/1563], Loss: 0.8919\n",
      "Epoch [12/20], Step [500/1563], Loss: 0.8533\n",
      "Epoch [12/20], Step [600/1563], Loss: 0.8730\n",
      "Epoch [12/20], Step [700/1563], Loss: 0.8472\n",
      "Epoch [12/20], Step [800/1563], Loss: 0.8449\n",
      "Epoch [12/20], Step [900/1563], Loss: 0.8793\n",
      "Epoch [12/20], Step [1000/1563], Loss: 0.9022\n",
      "Epoch [12/20], Step [1100/1563], Loss: 0.8715\n",
      "Epoch [12/20], Step [1200/1563], Loss: 0.9058\n",
      "Epoch [12/20], Step [1300/1563], Loss: 0.9084\n",
      "Epoch [12/20], Step [1400/1563], Loss: 0.8336\n",
      "Epoch [12/20], Step [1500/1563], Loss: 0.9073\n",
      "Epoch [13/20], Step [100/1563], Loss: 0.8077\n",
      "Epoch [13/20], Step [200/1563], Loss: 0.8070\n",
      "Epoch [13/20], Step [300/1563], Loss: 0.8137\n",
      "Epoch [13/20], Step [400/1563], Loss: 0.8504\n",
      "Epoch [13/20], Step [500/1563], Loss: 0.8294\n",
      "Epoch [13/20], Step [600/1563], Loss: 0.8377\n",
      "Epoch [13/20], Step [700/1563], Loss: 0.8313\n",
      "Epoch [13/20], Step [800/1563], Loss: 0.8300\n",
      "Epoch [13/20], Step [900/1563], Loss: 0.8559\n",
      "Epoch [13/20], Step [1000/1563], Loss: 0.8232\n",
      "Epoch [13/20], Step [1100/1563], Loss: 0.8761\n",
      "Epoch [13/20], Step [1200/1563], Loss: 0.8311\n",
      "Epoch [13/20], Step [1300/1563], Loss: 0.8465\n",
      "Epoch [13/20], Step [1400/1563], Loss: 0.8655\n",
      "Epoch [13/20], Step [1500/1563], Loss: 0.8271\n",
      "Epoch [14/20], Step [100/1563], Loss: 0.7766\n",
      "Epoch [14/20], Step [200/1563], Loss: 0.8115\n",
      "Epoch [14/20], Step [300/1563], Loss: 0.8199\n",
      "Epoch [14/20], Step [400/1563], Loss: 0.8118\n",
      "Epoch [14/20], Step [500/1563], Loss: 0.7789\n",
      "Epoch [14/20], Step [600/1563], Loss: 0.8315\n",
      "Epoch [14/20], Step [700/1563], Loss: 0.7903\n",
      "Epoch [14/20], Step [800/1563], Loss: 0.8153\n",
      "Epoch [14/20], Step [900/1563], Loss: 0.8106\n",
      "Epoch [14/20], Step [1000/1563], Loss: 0.8030\n",
      "Epoch [14/20], Step [1100/1563], Loss: 0.8073\n",
      "Epoch [14/20], Step [1200/1563], Loss: 0.8015\n",
      "Epoch [14/20], Step [1300/1563], Loss: 0.8396\n",
      "Epoch [14/20], Step [1400/1563], Loss: 0.8564\n",
      "Epoch [14/20], Step [1500/1563], Loss: 0.8304\n",
      "Epoch [15/20], Step [100/1563], Loss: 0.7436\n",
      "Epoch [15/20], Step [200/1563], Loss: 0.7614\n",
      "Epoch [15/20], Step [300/1563], Loss: 0.7940\n",
      "Epoch [15/20], Step [400/1563], Loss: 0.7677\n",
      "Epoch [15/20], Step [500/1563], Loss: 0.7819\n",
      "Epoch [15/20], Step [600/1563], Loss: 0.7672\n",
      "Epoch [15/20], Step [700/1563], Loss: 0.8066\n",
      "Epoch [15/20], Step [800/1563], Loss: 0.8068\n",
      "Epoch [15/20], Step [900/1563], Loss: 0.7797\n",
      "Epoch [15/20], Step [1000/1563], Loss: 0.7676\n",
      "Epoch [15/20], Step [1100/1563], Loss: 0.8129\n",
      "Epoch [15/20], Step [1200/1563], Loss: 0.8227\n",
      "Epoch [15/20], Step [1300/1563], Loss: 0.7901\n",
      "Epoch [15/20], Step [1400/1563], Loss: 0.8286\n",
      "Epoch [15/20], Step [1500/1563], Loss: 0.8248\n",
      "Epoch [16/20], Step [100/1563], Loss: 0.7306\n",
      "Epoch [16/20], Step [200/1563], Loss: 0.7145\n",
      "Epoch [16/20], Step [300/1563], Loss: 0.7907\n",
      "Epoch [16/20], Step [400/1563], Loss: 0.7631\n",
      "Epoch [16/20], Step [500/1563], Loss: 0.7715\n",
      "Epoch [16/20], Step [600/1563], Loss: 0.7679\n",
      "Epoch [16/20], Step [700/1563], Loss: 0.7384\n",
      "Epoch [16/20], Step [800/1563], Loss: 0.7597\n",
      "Epoch [16/20], Step [900/1563], Loss: 0.7704\n",
      "Epoch [16/20], Step [1000/1563], Loss: 0.7586\n",
      "Epoch [16/20], Step [1100/1563], Loss: 0.8089\n",
      "Epoch [16/20], Step [1200/1563], Loss: 0.8010\n",
      "Epoch [16/20], Step [1300/1563], Loss: 0.8005\n",
      "Epoch [16/20], Step [1400/1563], Loss: 0.7728\n",
      "Epoch [16/20], Step [1500/1563], Loss: 0.7862\n",
      "Epoch [17/20], Step [100/1563], Loss: 0.6638\n",
      "Epoch [17/20], Step [200/1563], Loss: 0.7161\n",
      "Epoch [17/20], Step [300/1563], Loss: 0.6890\n",
      "Epoch [17/20], Step [400/1563], Loss: 0.7494\n",
      "Epoch [17/20], Step [500/1563], Loss: 0.7696\n",
      "Epoch [17/20], Step [600/1563], Loss: 0.6963\n",
      "Epoch [17/20], Step [700/1563], Loss: 0.7528\n",
      "Epoch [17/20], Step [800/1563], Loss: 0.7640\n",
      "Epoch [17/20], Step [900/1563], Loss: 0.7466\n",
      "Epoch [17/20], Step [1000/1563], Loss: 0.7516\n",
      "Epoch [17/20], Step [1100/1563], Loss: 0.7705\n",
      "Epoch [17/20], Step [1200/1563], Loss: 0.7725\n",
      "Epoch [17/20], Step [1300/1563], Loss: 0.8042\n",
      "Epoch [17/20], Step [1400/1563], Loss: 0.7744\n",
      "Epoch [17/20], Step [1500/1563], Loss: 0.7678\n",
      "Epoch [18/20], Step [100/1563], Loss: 0.6744\n",
      "Epoch [18/20], Step [200/1563], Loss: 0.6629\n",
      "Epoch [18/20], Step [300/1563], Loss: 0.7067\n",
      "Epoch [18/20], Step [400/1563], Loss: 0.6892\n",
      "Epoch [18/20], Step [500/1563], Loss: 0.7204\n",
      "Epoch [18/20], Step [600/1563], Loss: 0.7147\n",
      "Epoch [18/20], Step [700/1563], Loss: 0.7289\n",
      "Epoch [18/20], Step [800/1563], Loss: 0.7321\n",
      "Epoch [18/20], Step [900/1563], Loss: 0.7171\n",
      "Epoch [18/20], Step [1000/1563], Loss: 0.7666\n",
      "Epoch [18/20], Step [1100/1563], Loss: 0.7600\n",
      "Epoch [18/20], Step [1200/1563], Loss: 0.7642\n",
      "Epoch [18/20], Step [1300/1563], Loss: 0.7447\n",
      "Epoch [18/20], Step [1400/1563], Loss: 0.7862\n",
      "Epoch [18/20], Step [1500/1563], Loss: 0.7658\n",
      "Epoch [19/20], Step [100/1563], Loss: 0.6629\n",
      "Epoch [19/20], Step [200/1563], Loss: 0.6690\n",
      "Epoch [19/20], Step [300/1563], Loss: 0.6934\n",
      "Epoch [19/20], Step [400/1563], Loss: 0.7025\n",
      "Epoch [19/20], Step [500/1563], Loss: 0.6919\n",
      "Epoch [19/20], Step [600/1563], Loss: 0.7027\n",
      "Epoch [19/20], Step [700/1563], Loss: 0.7306\n",
      "Epoch [19/20], Step [800/1563], Loss: 0.7127\n",
      "Epoch [19/20], Step [900/1563], Loss: 0.7314\n",
      "Epoch [19/20], Step [1000/1563], Loss: 0.7199\n",
      "Epoch [19/20], Step [1100/1563], Loss: 0.7154\n",
      "Epoch [19/20], Step [1200/1563], Loss: 0.7689\n",
      "Epoch [19/20], Step [1300/1563], Loss: 0.7393\n",
      "Epoch [19/20], Step [1400/1563], Loss: 0.7399\n",
      "Epoch [19/20], Step [1500/1563], Loss: 0.7140\n",
      "Epoch [20/20], Step [100/1563], Loss: 0.6627\n",
      "Epoch [20/20], Step [200/1563], Loss: 0.6465\n",
      "Epoch [20/20], Step [300/1563], Loss: 0.6583\n",
      "Epoch [20/20], Step [400/1563], Loss: 0.6721\n",
      "Epoch [20/20], Step [500/1563], Loss: 0.6775\n",
      "Epoch [20/20], Step [600/1563], Loss: 0.6846\n",
      "Epoch [20/20], Step [700/1563], Loss: 0.7014\n",
      "Epoch [20/20], Step [800/1563], Loss: 0.6886\n",
      "Epoch [20/20], Step [900/1563], Loss: 0.7259\n",
      "Epoch [20/20], Step [1000/1563], Loss: 0.7107\n",
      "Epoch [20/20], Step [1100/1563], Loss: 0.7178\n",
      "Epoch [20/20], Step [1200/1563], Loss: 0.7022\n",
      "Epoch [20/20], Step [1300/1563], Loss: 0.7039\n",
      "Epoch [20/20], Step [1400/1563], Loss: 0.7220\n",
      "Epoch [20/20], Step [1500/1563], Loss: 0.7255\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Set the number of epochs for training\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get the inputs and move them to the GPU if available\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # Print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 61.73%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pruning(model, method='global', amount=0.2):\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    if method == 'global':\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=amount,\n",
    "        )\n",
    "    elif method == 'l1_unstructured':\n",
    "        for module, param in parameters_to_prune:\n",
    "            prune.l1_unstructured(module, name=param, amount=amount)\n",
    "    elif method == 'random_unstructured':\n",
    "        for module, param in parameters_to_prune:\n",
    "            prune.random_unstructured(module, name=param, amount=amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeNet.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/serialization.py:883\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 883\u001b[0m     storage \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[0;32m~/miniconda3/envs/rakuten_project/lib/python3.11/site-packages/torch/storage.py:167\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mUntypedStorage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),\"LeNet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Function to calculate the sparsity of the pruned layers\n",
    "def calculate_sparsity(model):\n",
    "    total_params = 0\n",
    "    pruned_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            total_params += module.weight.nelement()  # Total number of weights\n",
    "            pruned_params += torch.sum(module.weight == 0).item()  # Pruned (zeroed-out) weights\n",
    "\n",
    "    return 100 * pruned_params / total_params\n",
    "\n",
    "\n",
    "# Test and print results for each pruning method\n",
    "pruning_methods = ['global', 'l1_unstructured', 'random_unstructured']\n",
    "\n",
    "for method in pruning_methods:\n",
    "    # Reset the model\n",
    "    \n",
    "    # Apply pruning\n",
    "    print(f\"\\nApplying {method} pruning:\")\n",
    "    apply_pruning(model, method=method, amount=0.2)\n",
    "\n",
    "    # Calculate and print sparsity\n",
    "    sparsity = calculate_sparsity(model)\n",
    "    print(f\"Sparsity after {method} pruning: {sparsity:.2f}%\")\n",
    "\n",
    "    # Optional: Inspect pruned weights in one of the layers (e.g., first conv layer)\n",
    "    print(f\"Sample pruned weights in conv1 layer after {method} pruning:\")\n",
    "    print(model.conv1.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
